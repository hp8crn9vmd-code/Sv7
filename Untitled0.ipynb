{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbY7DLkDe9jSEgQxwRm0pp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hp8crn9vmd-code/Sv7/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ndaqsCkUYV1",
        "outputId": "5f0c3b28-f1d4-4b60-876e-0e114b98d368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4266143053.py:4: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING: Library Audit & Harmonization...\n",
            "\n",
            "ğŸ” STEP 1: Current Environment Audit:\n",
            "   â€¢ numpy          : 2.0.2\n",
            "   â€¢ scipy          : 1.16.3\n",
            "   â€¢ paddlepaddle   : NOT INSTALLED\n",
            "   â€¢ paddleocr      : NOT INSTALLED\n",
            "   â€¢ ultralytics    : NOT INSTALLED\n",
            "\n",
            "ğŸ§¹ STEP 2: Removing Conflicting Libraries...\n",
            "\n",
            "ğŸ”§ STEP 3: Installing Harmonized Stack...\n",
            "\n",
            "âœ… PHASE 1 COMPLETE.\n",
            "âš ï¸ CRITICAL: You MUST restart the session now to load the new Numpy.\n",
            "   ACTION: Go to 'Runtime' -> 'Restart Session', then run the next cell.\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# PHASE 1: AUDIT, CLEANSE & HARMONIZE\n",
        "# ======================================================\n",
        "import pkg_resources\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "print(\"ğŸš€ STARTING: Library Audit & Harmonization...\")\n",
        "\n",
        "# 1. Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø© Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ ÙØ­ØµÙ‡Ø§\n",
        "critical_libs = [\"numpy\", \"scipy\", \"paddlepaddle\", \"paddleocr\", \"ultralytics\"]\n",
        "\n",
        "print(\"\\nğŸ” STEP 1: Current Environment Audit:\")\n",
        "try:\n",
        "    for lib in critical_libs:\n",
        "        try:\n",
        "            ver = pkg_resources.get_distribution(lib).version\n",
        "            print(f\"   â€¢ {lib:<15}: {ver}\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"   â€¢ {lib:<15}: NOT INSTALLED\")\n",
        "except Exception as e:\n",
        "    print(f\"   (Audit Info: {e})\")\n",
        "\n",
        "# 2. Ø§Ù„ØªØ®Ù„Øµ Ù…Ù† Ø§Ù„ØªØ¹Ø§Ø±Ø¶Ø§Øª (The Purge)\n",
        "print(\"\\nğŸ§¹ STEP 2: Removing Conflicting Libraries...\")\n",
        "# Ù†Ù‚ÙˆÙ… Ø¨Ø­Ø°Ù Ø£ÙŠ Ù†Ø³Ø®Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ø¶Ù…Ø§Ù† ØªØ«Ø¨ÙŠØª Ù†Ø¸ÙŠÙ\n",
        "subprocess.check_call([\n",
        "    sys.executable, \"-m\", \"pip\", \"uninstall\",\n",
        "    \"numpy\", \"scipy\", \"paddlepaddle\", \"paddleocr\", \"paddlepaddle-gpu\",\n",
        "    \"-y\"\n",
        "])\n",
        "\n",
        "# 3. ØªØ«Ø¨ÙŠØª Ø§Ù„Ø­Ø²Ù…Ø© Ø§Ù„Ø°Ù‡Ø¨ÙŠØ© (The Golden Stack)\n",
        "# Ù‡Ø°Ù‡ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª ØªÙ… Ø§Ø®ØªÙŠØ§Ø±Ù‡Ø§ Ø¨Ø¯Ù‚Ø© Ù„ØªØ¹Ù…Ù„ Ù…Ø¹Ø§Ù‹ Ø¨Ø¯ÙˆÙ† Ø£Ø®Ø·Ø§Ø¡\n",
        "print(\"\\nğŸ”§ STEP 3: Installing Harmonized Stack...\")\n",
        "subprocess.check_call([\n",
        "    sys.executable, \"-m\", \"pip\", \"install\",\n",
        "    \"numpy==1.26.4\",           # Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©: Ø¥ØµØ¯Ø§Ø± Ù…Ø³ØªÙ‚Ø± ÙŠÙ…Ù†Ø¹ Ø§Ù†Ù‡ÙŠØ§Ø± Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n",
        "    \"scipy==1.12.0\",           # Ø§Ù„ØªÙˆØ§ÙÙ‚: Ù…ØªÙˆØ§ÙÙ‚ ØªÙ…Ø§Ù…Ø§Ù‹ Ù…Ø¹ Numpy 1.26\n",
        "    \"paddlepaddle==2.6.2\",     # Ø§Ù„Ù…Ø­Ø±Ùƒ: Ù†Ø³Ø®Ø© CPU Ø§Ù„Ù…Ø³ØªÙ‚Ø±Ø©\n",
        "    \"paddleocr==2.7.3\",        # Ø§Ù„Ù‚Ø§Ø±Ø¦: Ø¥ØµØ¯Ø§Ø± Ù†Ø§Ø¶Ø¬ (Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø£Ø®Ø·Ø§Ø¡ API Ø§Ù„Ø­Ø¯ÙŠØ«Ø©)\n",
        "    \"ultralytics>=8.3.0\",      # Ø§Ù„Ø±Ø¤ÙŠØ©: Ø£Ø­Ø¯Ø« Ù†Ø³Ø®Ø© (Ù…ØªÙˆØ§ÙÙ‚Ø©)\n",
        "    \"langchain<0.2.0\",         # Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
        "    \"langchain-community<0.2.0\"\n",
        "])\n",
        "\n",
        "print(\"\\nâœ… PHASE 1 COMPLETE.\")\n",
        "print(\"âš ï¸ CRITICAL: You MUST restart the session now to load the new Numpy.\")\n",
        "print(\"   ACTION: Go to 'Runtime' -> 'Restart Session', then run the next cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# PHASE 2: VERIFICATION & FINAL ACTIVATION\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "import paddle\n",
        "from paddleocr import PaddleOCR\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"ğŸš€ STARTING: Phase 2 (Verification & Launch)...\")\n",
        "\n",
        "# 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory Check)\n",
        "# Ù†ØªØ£ÙƒØ¯ Ø£Ù† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ´ØºÙŠÙ„ Ù‚Ø¯ Ø·Ø±Ø¯Øª Numpy 2.0\n",
        "import numpy\n",
        "print(f\"   ğŸ” Loaded Numpy Version: {numpy.__version__}\", end=\" \")\n",
        "\n",
        "if numpy.__version__.startswith(\"2\"):\n",
        "    print(\"âŒ CRITICAL FAIL\")\n",
        "    print(\"   âš ï¸ The session was NOT restarted correctly.\")\n",
        "    print(\"   ğŸ‘‰ Please go to 'Runtime' -> 'Restart Session' and run this cell again.\")\n",
        "    sys.exit()\n",
        "else:\n",
        "    print(\"âœ… (Safe & Compatible)\")\n",
        "\n",
        "# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø¹Ù…Ù„\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "BASE_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2'\n",
        "DATA_DIR = os.path.join(BASE_DIR, 'data/raw')\n",
        "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "# 3. Ù‚ÙÙ„ Ø§Ù„Ø¹ØªØ§Ø¯ (CPU Lock)\n",
        "paddle.set_device('cpu')\n",
        "print(\"   âœ… Hardware: CPU Mode (Locked)\")\n",
        "\n",
        "# 4. ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø±Ø¤ÙŠØ© (YOLO)\n",
        "print(\"\\nâ¬‡ï¸ Activating Vision Module...\")\n",
        "if not os.path.exists(os.path.join(MODELS_DIR, 'yolov11m.pt')):\n",
        "    YOLO('yolo11m.pt').save(os.path.join(MODELS_DIR, 'yolov11m.pt'))\n",
        "print(\"   âœ… Vision Engine: ONLINE\")\n",
        "\n",
        "# 5. ØªÙØ¹ÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ (PaddleOCR)\n",
        "print(\"\\nâ¬‡ï¸ Activating Text Module...\")\n",
        "\n",
        "try:\n",
        "    # Ø¨Ù…Ø§ Ø£Ù†Ù†Ø§ Ø«Ø¨ØªÙ†Ø§ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…Ø³ØªÙ‚Ø± 2.7.3ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ø¶Ù…ÙˆÙ†:\n",
        "    ocr_engine = PaddleOCR(\n",
        "        lang='en',             # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ ÙÙ‚Ø·\n",
        "        use_angle_cls=False,   # Ø¥ÙŠÙ‚Ø§Ù Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø³Ø±Ø¹Ø©\n",
        "        use_gpu=False,         # ØªØ£ÙƒÙŠØ¯ ÙˆØ¶Ø¹ CPU\n",
        "        show_log=False,        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª\n",
        "        enable_mkldnn=False    # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø±ÙŠØ¹ Ù„Ù…Ù†Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
        "    )\n",
        "    print(\"   âœ… Text Engine: ONLINE (Golden Stack Config)\")\n",
        "\n",
        "    print(\"\\nğŸ‰ SOVEREIGN ENGINE IS FULLY OPERATIONAL.\")\n",
        "    print(f\"   ğŸ“‚ Ready to process PDFs in: {DATA_DIR}\")\n",
        "\n",
        "    # Ø­ÙØ¸ Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¬Ø§Ø­\n",
        "    with open(\"config.yaml\", \"w\") as f:\n",
        "        f.write(\"status: operational\\nstack: golden_v2.7.3\\nprofile: cpu_stable\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ ERROR: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNWIJ-psVC3Q",
        "outputId": "5c76bced-25c5-4154-ea14-0c6b31898019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "ğŸš€ STARTING: Phase 2 (Verification & Launch)...\n",
            "   ğŸ” Loaded Numpy Version: 1.26.4 âœ… (Safe & Compatible)\n",
            "Mounted at /content/drive\n",
            "   âœ… Hardware: CPU Mode (Locked)\n",
            "\n",
            "â¬‡ï¸ Activating Vision Module...\n",
            "   âœ… Vision Engine: ONLINE\n",
            "\n",
            "â¬‡ï¸ Activating Text Module...\n",
            "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.00M/4.00M [00:17<00:00, 229kiB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.2M/10.2M [00:41<00:00, 249kiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.19M/2.19M [00:08<00:00, 246kiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âœ… Text Engine: ONLINE (Golden Stack Config)\n",
            "\n",
            "ğŸ‰ SOVEREIGN ENGINE IS FULLY OPERATIONAL.\n",
            "   ğŸ“‚ Ready to process PDFs in: /content/drive/MyDrive/sovereign-engine-v7.2/data/raw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# SOVEREIGN ENGINE V7.2 - FINAL VALIDATION (CORRECTED)\n",
        "# ======================================================\n",
        "import os\n",
        "import sys\n",
        "import requests\n",
        "\n",
        "print(\"ğŸš€ STARTING: Final Validation Protocol...\")\n",
        "\n",
        "# 1. ØªØ«Ø¨ÙŠØª Ù…ÙƒØªØ¨Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª PDF (Ø£ÙˆÙ„Ø§Ù‹)\n",
        "print(\"ğŸ”§ Installing PDF Generator...\")\n",
        "os.system(\"pip install reportlab --quiet\")\n",
        "\n",
        "# Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¨Ø£Ù…Ø§Ù†\n",
        "from reportlab.pdfgen import canvas\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "BASE_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2'\n",
        "RAW_DIR = os.path.join(BASE_DIR, 'data/raw')\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, 'data/processed')\n",
        "\n",
        "# 3. ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙØ§Ø³Ø¯Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©\n",
        "print(\"ğŸ§¹ Cleaning raw directory...\")\n",
        "for f in os.listdir(RAW_DIR):\n",
        "    try:\n",
        "        os.remove(os.path.join(RAW_DIR, f))\n",
        "    except: pass\n",
        "\n",
        "# 4. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PDF Ù…Ø­Ù„ÙŠ Ù†Ø¸ÙŠÙ (ÙØ§ØªÙˆØ±Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ©)\n",
        "valid_pdf_path = os.path.join(RAW_DIR, \"test_invoice.pdf\")\n",
        "print(\"ğŸ“ Generating Valid PDF Invoice...\")\n",
        "\n",
        "def create_valid_pdf(path):\n",
        "    c = canvas.Canvas(path)\n",
        "    c.setFont(\"Helvetica-Bold\", 16)\n",
        "    c.drawString(100, 800, \"INVOICE #SOV-72\")\n",
        "    c.setFont(\"Helvetica\", 12)\n",
        "    c.drawString(100, 780, \"Date: 2025-12-06\")\n",
        "    c.drawString(100, 750, \"BILL TO: Gemini User\")\n",
        "    c.line(100, 740, 500, 740)\n",
        "    c.drawString(100, 720, \"DESCRIPTION                  AMOUNT\")\n",
        "    c.drawString(100, 700, \"AI Engine Deployment         $5000.00\")\n",
        "    c.drawString(100, 680, \"Data Optimization            $2000.00\")\n",
        "    c.drawString(100, 640, \"TOTAL                        $7000.00\")\n",
        "    c.save()\n",
        "\n",
        "create_valid_pdf(valid_pdf_path)\n",
        "print(f\"   âœ… Created: {valid_pdf_path}\")\n",
        "\n",
        "# 5. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)\n",
        "if 'process_pdf' in globals():\n",
        "    print(\"\\nâš™ï¸ RUNNING PIPELINE...\")\n",
        "\n",
        "    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø± Ù„ÙƒØ§Ø¦Ù† Path ÙƒÙ…Ø§ ØªØªÙˆÙ‚Ø¹ Ø§Ù„Ø¯Ø§Ù„Ø©\n",
        "    pdf_file = Path(valid_pdf_path)\n",
        "\n",
        "    # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
        "    result = process_pdf(pdf_file)\n",
        "\n",
        "    if result:\n",
        "        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "        out_file = os.path.join(PROCESSED_DIR, pdf_file.stem + \".json\")\n",
        "        with open(out_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nâœ… SUCCESS! Data saved to: {out_file}\")\n",
        "        print(\"\\nğŸ“Š EXTRACTED TEXT PREVIEW:\")\n",
        "        print(\"=\" * 40)\n",
        "        # Ø·Ø¨Ø§Ø¹Ø© Ø£ÙˆÙ„ 300 Ø­Ø±Ù Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬\n",
        "        print(result['pages'][0]['full_text'][:300])\n",
        "        print(\"=\" * 40)\n",
        "        print(\"\\nğŸ‰ MISSION ACCOMPLISHED.\")\n",
        "    else:\n",
        "        print(\"âŒ Processing returned empty result.\")\n",
        "else:\n",
        "    print(\"âŒ Error: 'process_pdf' function not found.\")\n",
        "    print(\"   Please run the previous 'Data Processing' cell again.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TZxgyLQXE5r",
        "outputId": "86b25a5a-0ffa-4da3-8b4a-c066572f5fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING: Final Validation Protocol...\n",
            "ğŸ”§ Installing PDF Generator...\n",
            "ğŸ§¹ Cleaning raw directory...\n",
            "ğŸ“ Generating Valid PDF Invoice...\n",
            "   âœ… Created: /content/drive/MyDrive/sovereign-engine-v7.2/data/raw/test_invoice.pdf\n",
            "\n",
            "âš™ï¸ RUNNING PIPELINE...\n",
            "\n",
            "ğŸ“„ Processing: test_invoice.pdf\n",
            "   > Analyzing Page 1...\n",
            "\n",
            "âœ… SUCCESS! Data saved to: /content/drive/MyDrive/sovereign-engine-v7.2/data/processed/test_invoice.json\n",
            "\n",
            "ğŸ“Š EXTRACTED TEXT PREVIEW:\n",
            "========================================\n",
            "INVOICE #SOV-72 Date: 2025-12-06 BILL TO: Gemini User DESCRIPTION AMOUNT Al Engine Deployment $5000.00 Data Optimization $2000.00 TOTAL $7000.00\n",
            "========================================\n",
            "\n",
            "ğŸ‰ MISSION ACCOMPLISHED.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# SOVEREIGN ENGINE V7.2 - CLEAN TRAINING DATA PROCESSOR\n",
        "# ======================================================\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from ultralytics import YOLO\n",
        "from paddleocr import PaddleOCR\n",
        "import paddle\n",
        "\n",
        "print(\"ğŸš€ PRODUCTION MODE: Advanced Filtering Active...\")\n",
        "\n",
        "# 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª\n",
        "BASE_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2'\n",
        "RAW_DIR = os.path.join(BASE_DIR, 'data/raw')\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, 'data/processed')\n",
        "ARCHIVE_DIR = os.path.join(BASE_DIR, 'data/archive')\n",
        "MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
        "\n",
        "for d in [RAW_DIR, PROCESSED_DIR, ARCHIVE_DIR, MODELS_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª (Ù…Ø¹ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±)\n",
        "if 'ocr_engine' not in globals():\n",
        "    print(\"   ğŸ”§ Initializing Engines...\")\n",
        "    paddle.set_device('cpu')\n",
        "\n",
        "    # ØªØ­Ù…ÙŠÙ„ YOLO\n",
        "    if not os.path.exists(os.path.join(MODELS_DIR, 'yolov11m.pt')):\n",
        "        model = YOLO('yolo11m.pt')\n",
        "        model.save(os.path.join(MODELS_DIR, 'yolov11m.pt'))\n",
        "    else:\n",
        "        model = YOLO(os.path.join(MODELS_DIR, 'yolov11m.pt'))\n",
        "\n",
        "    # ØªØ­Ù…ÙŠÙ„ PaddleOCR (Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¢Ù…Ù†Ø©)\n",
        "    ocr_engine = PaddleOCR(\n",
        "        lang='en',\n",
        "        use_angle_cls=False,     # Ø¥ÙŠÙ‚Ø§Ù Ù…ØµØ­Ø­ Ø§Ù„Ø²ÙˆØ§ÙŠØ§ Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§Ù†Ù‡ÙŠØ§Ø±\n",
        "        use_gpu=False,           # ÙˆØ¶Ø¹ CPU\n",
        "        enable_mkldnn=False,     # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø²Ø§Ø¦Ø¯\n",
        "        show_log=False\n",
        "    )\n",
        "\n",
        "# 3. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙÙ„ØªØ±Ø© (Governance Config)\n",
        "# Ù‡Ø°Ù‡ Ø§Ù„Ù‚ÙŠÙ… ØªØ­Ø¯Ø¯ Ù…Ø³Ø§Ø­Ø© \"Ø§Ù„Ø­Ø¸Ø±\"\n",
        "MARGIN_TOP = 0.10     # ØªØ¬Ø§Ù‡Ù„ Ø£Ø¹Ù„Ù‰ 10% Ù…Ù† Ø§Ù„ØµÙØ­Ø© (Ù„Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†)\n",
        "MARGIN_BOTTOM = 0.10  # ØªØ¬Ø§Ù‡Ù„ Ø£Ø³ÙÙ„ 10% Ù…Ù† Ø§Ù„ØµÙØ­Ø© (Ù„Ø¥Ø²Ø§Ù„Ø© Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØµÙØ­Ø§Øª)\n",
        "\n",
        "def is_in_margin(box, img_height):\n",
        "    \"\"\"\n",
        "    ØªØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ ÙŠÙ‚Ø¹ ÙÙŠ Ù…Ù†Ø·Ù‚Ø© Ù…Ø­Ø¸ÙˆØ±Ø© (Ù‡Ø§Ù…Ø´ Ø¹Ù„ÙˆÙŠ Ø£Ùˆ Ø³ÙÙ„ÙŠ)\n",
        "    \"\"\"\n",
        "    # Ø­Ø³Ø§Ø¨ Ù…Ø±ÙƒØ² Ø§Ù„Ù†Øµ Ø¹Ù…ÙˆØ¯ÙŠØ§Ù‹ (Y-Coordinate)\n",
        "    # box format: [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]\n",
        "    y_center = sum([p[1] for p in box]) / 4\n",
        "\n",
        "    # Ù‡Ù„ Ù‡Ùˆ ÙÙŠ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø¹Ù„ÙˆÙŠØŸ\n",
        "    if y_center < (img_height * MARGIN_TOP):\n",
        "        return True\n",
        "\n",
        "    # Ù‡Ù„ Ù‡Ùˆ ÙÙŠ Ø§Ù„Ù‡Ø§Ù…Ø´ Ø§Ù„Ø³ÙÙ„ÙŠØŸ\n",
        "    if y_center > (img_height * (1 - MARGIN_BOTTOM)):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# 4. Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©\n",
        "new_files = list(Path(RAW_DIR).glob(\"*.pdf\"))\n",
        "\n",
        "if not new_files:\n",
        "    print(f\"   â„¹ï¸ No new PDFs in '{RAW_DIR}'. Upload files to start.\")\n",
        "else:\n",
        "    print(f\"   ğŸ“„ Found {len(new_files)} new file(s).\")\n",
        "\n",
        "    for pdf_path in new_files:\n",
        "        try:\n",
        "            print(f\"\\n   âš™ï¸ Processing: {pdf_path.name}\")\n",
        "\n",
        "            # ØªØ­ÙˆÙŠÙ„ PDF Ù„ØµÙˆØ±\n",
        "            try:\n",
        "                images = convert_from_path(str(pdf_path))\n",
        "            except Exception as e:\n",
        "                print(f\"      âŒ PDF Read Error (install poppler?): {e}\")\n",
        "                continue\n",
        "\n",
        "            doc_data = {\"filename\": pdf_path.name, \"pages\": []}\n",
        "\n",
        "            for i, image in enumerate(images):\n",
        "                img_np = np.array(image)\n",
        "                img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "                height, width, _ = img_np.shape\n",
        "\n",
        "                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ\n",
        "                ocr_out = ocr_engine.ocr(img_cv, cls=False)\n",
        "\n",
        "                clean_text_lines = []\n",
        "                ignored_count = 0\n",
        "\n",
        "                if ocr_out and ocr_out[0]:\n",
        "                    for line in ocr_out[0]:\n",
        "                        text_box = line[0]   # Ø§Ù„Ø¥Ø­Ø¯Ø§Ø«ÙŠØ§Øª\n",
        "                        text_str = line[1][0] # Ø§Ù„Ù†Øµ\n",
        "                        confidence = line[1][1]\n",
        "\n",
        "                        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„ØªØ± Ø§Ù„Ù…ÙƒØ§Ù†ÙŠ\n",
        "                        if is_in_margin(text_box, height):\n",
        "                            ignored_count += 1\n",
        "                            continue # ØªØ¬Ø§Ù‡Ù„ (Ø¹Ù†ÙˆØ§Ù†/Ø±Ù‚Ù… ØµÙØ­Ø©)\n",
        "\n",
        "                        # ØªØµÙÙŠØ© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¶Ø¹ÙŠÙØ©\n",
        "                        if confidence > 0.60:\n",
        "                            clean_text_lines.append(text_str)\n",
        "\n",
        "                final_text = \" \".join(clean_text_lines)\n",
        "\n",
        "                doc_data[\"pages\"].append({\n",
        "                    \"page_num\": i + 1,\n",
        "                    \"clean_text\": final_text,\n",
        "                    \"stats\": {\n",
        "                        \"ignored_elements\": ignored_count,\n",
        "                        \"valid_elements\": len(clean_text_lines)\n",
        "                    }\n",
        "                })\n",
        "                print(f\"      > Page {i+1}: Extracted {len(clean_text_lines)} lines (Filtered {ignored_count} margin items).\")\n",
        "\n",
        "            # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "            json_path = os.path.join(PROCESSED_DIR, pdf_path.stem + \".json\")\n",
        "            with open(json_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(doc_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "            print(f\"      âœ… Data Cleaned & Saved: {json_path}\")\n",
        "\n",
        "            # Ø£Ø±Ø´ÙØ© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ\n",
        "            shutil.move(str(pdf_path), os.path.join(ARCHIVE_DIR, pdf_path.name))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      âŒ Error processing {pdf_path.name}: {e}\")\n",
        "\n",
        "    print(\"\\nğŸ‰ CLEANING COMPLETE. READY FOR TRAINING.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRVx11ipY_KC",
        "outputId": "e8ff48cf-f386-4ade-e28e-410fdd4a0637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ PRODUCTION MODE: Advanced Filtering Active...\n",
            "   ğŸ“„ Found 1 new file(s).\n",
            "\n",
            "   âš™ï¸ Processing: test_invoice.pdf\n",
            "      > Page 1: Extracted 9 lines (Filtered 2 margin items).\n",
            "      âœ… Data Cleaned & Saved: /content/drive/MyDrive/sovereign-engine-v7.2/data/processed/test_invoice.json\n",
            "\n",
            "ğŸ‰ CLEANING COMPLETE. READY FOR TRAINING.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# MODULE 1: BATCH PROCESSING (PDF -> CLEAN JSON)\n",
        "# ======================================================\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import shutil\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from pdf2image import convert_from_path\n",
        "from ultralytics import YOLO\n",
        "from paddleocr import PaddleOCR\n",
        "import paddle\n",
        "\n",
        "print(\"ğŸš€ STARTING: Real Data Processing Pipeline...\")\n",
        "\n",
        "# 1. Setup Paths\n",
        "BASE_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2'\n",
        "RAW_DIR = os.path.join(BASE_DIR, 'data/raw')\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, 'data/processed')\n",
        "ARCHIVE_DIR = os.path.join(BASE_DIR, 'data/archive')\n",
        "\n",
        "for d in [RAW_DIR, PROCESSED_DIR, ARCHIVE_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 2. Check for files\n",
        "pdf_files = list(Path(RAW_DIR).glob(\"*.pdf\"))\n",
        "if not pdf_files:\n",
        "    print(f\"   âš ï¸ No PDFs found in {RAW_DIR}\")\n",
        "    print(\"   Please upload your files and run this cell again.\")\n",
        "else:\n",
        "    print(f\"   ğŸ“„ Found {len(pdf_files)} document(s) to process.\")\n",
        "\n",
        "    # 3. Initialize Engines (CPU Mode)\n",
        "    paddle.set_device('cpu')\n",
        "    print(\"   ğŸ”§ Engines Initialized (CPU Mode).\")\n",
        "\n",
        "    # Load YOLO\n",
        "    model_path = os.path.join(BASE_DIR, 'models/yolov11m.pt')\n",
        "    if not os.path.exists(model_path):\n",
        "        from ultralytics import YOLO\n",
        "        YOLO('yolo11m.pt').save(model_path)\n",
        "    model = YOLO(model_path)\n",
        "\n",
        "    # Load PaddleOCR (Safe Config)\n",
        "    ocr_engine = PaddleOCR(\n",
        "        lang='en',\n",
        "        use_angle_cls=False,\n",
        "        use_gpu=False,\n",
        "        enable_mkldnn=False,\n",
        "        show_log=False\n",
        "    )\n",
        "\n",
        "    # 4. Processing Loop\n",
        "    MARGIN_TOP = 0.10\n",
        "    MARGIN_BOTTOM = 0.10\n",
        "\n",
        "    def is_header_footer(box, h):\n",
        "        y = sum([p[1] for p in box]) / 4\n",
        "        return y < (h * MARGIN_TOP) or y > (h * (1 - MARGIN_BOTTOM))\n",
        "\n",
        "    for pdf in pdf_files:\n",
        "        try:\n",
        "            print(f\"\\n   âš™ï¸ Processing: {pdf.name}\")\n",
        "            images = convert_from_path(str(pdf))\n",
        "            doc_data = {\"filename\": pdf.name, \"pages\": []}\n",
        "\n",
        "            for i, img in enumerate(images):\n",
        "                img_np = np.array(img)\n",
        "                img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "                h, w, _ = img_np.shape\n",
        "\n",
        "                # Extract Text\n",
        "                result = ocr_engine.ocr(img_cv, cls=False)\n",
        "                clean_lines = []\n",
        "\n",
        "                if result and result[0]:\n",
        "                    for line in result[0]:\n",
        "                        box, (text, conf) = line[0], line[1]\n",
        "                        if conf > 0.60 and not is_header_footer(box, h):\n",
        "                            clean_lines.append(text)\n",
        "\n",
        "                page_text = \" \".join(clean_lines)\n",
        "\n",
        "                # Save Page Data if it has content\n",
        "                if len(page_text) > 50:\n",
        "                    doc_data[\"pages\"].append({\n",
        "                        \"page_num\": i + 1,\n",
        "                        \"text\": page_text\n",
        "                    })\n",
        "                    print(f\"      > Page {i+1}: Extracted {len(page_text)} chars.\")\n",
        "\n",
        "            # Save JSON\n",
        "            if doc_data[\"pages\"]:\n",
        "                out_file = os.path.join(PROCESSED_DIR, pdf.stem + \".json\")\n",
        "                with open(out_file, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(doc_data, f, indent=4, ensure_ascii=False)\n",
        "                print(f\"      âœ… Saved: {out_file}\")\n",
        "\n",
        "                # Archive original\n",
        "                shutil.move(str(pdf), os.path.join(ARCHIVE_DIR, pdf.name))\n",
        "            else:\n",
        "                print(\"      âš ï¸ Skipped (No valid text found)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"      âŒ Error: {e}\")\n",
        "\n",
        "    print(\"\\nğŸ‰ PROCESSING COMPLETE.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrhOl0P-Zhmg",
        "outputId": "e3b3d13b-31c0-4634-d7d8-83459fe93890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING: Real Data Processing Pipeline...\n",
            "   âš ï¸ No PDFs found in /content/drive/MyDrive/sovereign-engine-v7.2/data/raw\n",
            "   Please upload your files and run this cell again.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# MODULE 2: DATASET FORMATTER (JSON -> JSONL)\n",
        "# ======================================================\n",
        "import glob\n",
        "\n",
        "print(\"ğŸš€ STARTING: Dataset Generation for Training...\")\n",
        "\n",
        "PROCESSED_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2/data/processed'\n",
        "DATASET_FILE = '/content/drive/MyDrive/sovereign-engine-v7.2/data/train_dataset.jsonl'\n",
        "\n",
        "json_files = glob.glob(os.path.join(PROCESSED_DIR, \"*.json\"))\n",
        "dataset_entries = []\n",
        "\n",
        "# Ø§Ù„Ù‚Ø§Ù„Ø¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ÙŠ (Prompt Template)\n",
        "system_prompt = \"You are a Sovereign AI specialized in analyzing documents. Extract and summarize the key information.\"\n",
        "\n",
        "print(f\"   ğŸ“‚ Found {len(json_files)} processed files.\")\n",
        "\n",
        "for j_file in json_files:\n",
        "    with open(j_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    filename = data.get('filename', 'Unknown Document')\n",
        "\n",
        "    for page in data.get('pages', []):\n",
        "        text_content = page.get('text', '')\n",
        "\n",
        "        # ØªØ®Ø·ÙŠ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ© Ø¬Ø¯Ø§Ù‹\n",
        "        if len(text_content) < 100: continue\n",
        "\n",
        "        # ØªØ´ÙƒÙŠÙ„ Ø¹ÙŠÙ†Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Unsloth Format)\n",
        "        # Ù†Ù‚ÙˆÙ… Ø¨ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¯Ø±Ø§Ù‹ Ø¹Ù„Ù‰ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰\n",
        "        entry = {\n",
        "            \"instruction\": f\"Analyze the content of the document: {filename}, Page {page['page_num']}.\",\n",
        "            \"input\": \"\",\n",
        "            \"output\": text_content\n",
        "        }\n",
        "        dataset_entries.append(entry)\n",
        "\n",
        "# Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ\n",
        "with open(DATASET_FILE, 'w', encoding='utf-8') as f:\n",
        "    for entry in dataset_entries:\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n",
        "\n",
        "print(f\"\\nâœ… DATASET CREATED: {DATASET_FILE}\")\n",
        "print(f\"   ğŸ“Š Total Training Samples: {len(dataset_entries)}\")\n",
        "print(\"   Ready for Module D (Unsloth Training).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKTnhG-JZrA_",
        "outputId": "5b52ec48-ed2b-49f8-c8bb-c7d02fb1374c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING: Dataset Generation for Training...\n",
            "   ğŸ“‚ Found 1 processed files.\n",
            "\n",
            "âœ… DATASET CREATED: /content/drive/MyDrive/sovereign-engine-v7.2/data/train_dataset.jsonl\n",
            "   ğŸ“Š Total Training Samples: 0\n",
            "   Ready for Module D (Unsloth Training).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# SOVEREIGN ENGINE V7.2 - PERMANENT PRODUCTION PIPELINE\n",
        "# ======================================================\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab import files, drive\n",
        "from pdf2image import convert_from_path\n",
        "from ultralytics import YOLO\n",
        "from paddleocr import PaddleOCR\n",
        "import paddle\n",
        "\n",
        "print(\"ğŸš€ STARTING: Secure Production Pipeline (Drive-Backed)...\")\n",
        "\n",
        "# 1. Ø±Ø¨Ø· Google Drive (Ù„Ù„Ø­ÙØ¸ Ø§Ù„Ø¯Ø§Ø¦Ù…)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¯Ø§Ø¦Ù…Ø©\n",
        "BASE_DIR = '/content/drive/MyDrive/sovereign-engine-v7.2'\n",
        "RAW_DIR = os.path.join(BASE_DIR, 'data/raw')\n",
        "PROCESSED_DIR = os.path.join(BASE_DIR, 'data/processed')\n",
        "DATASET_FILE = os.path.join(BASE_DIR, 'data/train_dataset.jsonl')\n",
        "\n",
        "for d in [RAW_DIR, PROCESSED_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "# 2. Ø±ÙØ¹ Ø§Ù„Ù…Ù„Ù (Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ ÙÙŠ Drive)\n",
        "print(\"\\nğŸ“¤ STEP 1: Upload Document...\")\n",
        "# Ù†ØªØ­Ù‚Ù‚ Ø£ÙˆÙ„Ø§Ù‹ Ù‡Ù„ Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ DriveØŸ\n",
        "existing_pdfs = [f for f in os.listdir(RAW_DIR) if f.endswith('.pdf')]\n",
        "target_pdf_path = None\n",
        "\n",
        "if existing_pdfs:\n",
        "    print(f\"   ğŸ“„ Found existing PDF in Drive: {existing_pdfs[0]}\")\n",
        "    use_existing = input(\"   Use this file? (y/n): \").lower()\n",
        "    if use_existing == 'y':\n",
        "        target_pdf_path = os.path.join(RAW_DIR, existing_pdfs[0])\n",
        "\n",
        "if not target_pdf_path:\n",
        "    print(\"   ğŸ‘‰ Please upload your Engineering Book (PDF) now:\")\n",
        "    uploaded = files.upload()\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        target_pdf_path = os.path.join(RAW_DIR, filename)\n",
        "        shutil.move(filename, target_pdf_path) # Ù†Ù‚Ù„Ù‡ Ù„Ù„Ø¯Ø±Ø§ÙŠÙ Ù„Ù„Ø£Ù…Ø§Ù†\n",
        "        print(f\"   âœ… Saved to Drive: {target_pdf_path}\")\n",
        "    else:\n",
        "        print(\"âŒ No file uploaded.\")\n",
        "        # ØªÙˆÙ‚Ù Ù…Ø¤Ù‚Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø±ÙØ¹ Ù…Ù„Ù\n",
        "\n",
        "if target_pdf_path:\n",
        "    # 3. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø±ÙƒØ§Øª (CPU Mode Ø§Ù„Ù…Ø³ØªÙ‚Ø±)\n",
        "    print(\"\\nğŸ”§ STEP 2: Initializing Engines...\")\n",
        "    paddle.set_device('cpu')\n",
        "\n",
        "    # ØªØ­Ù…ÙŠÙ„ Ù†Ù…Ø§Ø°Ø¬\n",
        "    MODELS_DIR = os.path.join(BASE_DIR, 'models')\n",
        "    if not os.path.exists(os.path.join(MODELS_DIR, 'yolov11m.pt')):\n",
        "        YOLO('yolo11m.pt').save(os.path.join(MODELS_DIR, 'yolov11m.pt'))\n",
        "    model = YOLO(os.path.join(MODELS_DIR, 'yolov11m.pt'))\n",
        "\n",
        "    ocr_engine = PaddleOCR(\n",
        "        lang='en',\n",
        "        use_angle_cls=False,\n",
        "        use_gpu=False,\n",
        "        enable_mkldnn=False,\n",
        "        show_log=False\n",
        "    )\n",
        "\n",
        "    # 4. Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Extraction)\n",
        "    print(f\"\\nâš™ï¸ STEP 3: Processing {os.path.basename(target_pdf_path)}...\")\n",
        "    try:\n",
        "        images = convert_from_path(target_pdf_path)\n",
        "        doc_data = {\"filename\": os.path.basename(target_pdf_path), \"pages\": []}\n",
        "\n",
        "        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ÙÙ„ØªØ±Ø©\n",
        "        MARGIN = 0.10 # 10% Ù‡ÙˆØ§Ù…Ø´\n",
        "\n",
        "        print(f\"   ğŸ“– Total Pages: {len(images)}\")\n",
        "\n",
        "        for i, image in enumerate(images):\n",
        "            if (i+1) % 5 == 0: print(f\"      Processing page {i+1}...\")\n",
        "\n",
        "            img_np = np.array(image)\n",
        "            img_cv = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
        "            h, w, _ = img_np.shape\n",
        "\n",
        "            result = ocr_engine.ocr(img_cv, cls=False)\n",
        "            clean_lines = []\n",
        "\n",
        "            if result and result[0]:\n",
        "                for line in result[0]:\n",
        "                    box, (text, conf) = line[0], line[1]\n",
        "                    # ÙÙ„ØªØ±Ø© Ø§Ù„Ù‡ÙˆØ§Ù…Ø´\n",
        "                    y_center = sum([p[1] for p in box]) / 4\n",
        "                    if conf > 0.60 and (h * MARGIN < y_center < h * (1 - MARGIN)):\n",
        "                        clean_lines.append(text)\n",
        "\n",
        "            page_text = \" \".join(clean_lines)\n",
        "\n",
        "            # Ø­ÙØ¸ Ø§Ù„ØµÙØ­Ø© ÙÙ‚Ø· Ø¥Ø°Ø§ ÙƒØ§Ù†Øª ØªØ­ØªÙˆÙŠ Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙÙŠØ¯Ø©\n",
        "            if len(page_text) > 50:\n",
        "                doc_data[\"pages\"].append({\n",
        "                    \"page_num\": i + 1,\n",
        "                    \"clean_text\": page_text\n",
        "                })\n",
        "\n",
        "        # Ø­ÙØ¸ JSON ÙÙŠ Drive (Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ø¯Ø§Ø¦Ù…)\n",
        "        json_output = os.path.join(PROCESSED_DIR, os.path.basename(target_pdf_path).replace('.pdf', '.json'))\n",
        "        with open(json_output, 'w', encoding='utf-8') as f:\n",
        "            json.dump(doc_data, f, indent=4, ensure_ascii=False)\n",
        "        print(f\"   âœ… JSON Saved to Drive: {json_output}\")\n",
        "\n",
        "        # 5. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ¯Ø±ÙŠØ¨ (Compilation)\n",
        "        print(\"\\nğŸ”¨ STEP 4: Compiling Training Dataset...\")\n",
        "        dataset_entries = []\n",
        "\n",
        "        # ØªÙ‚Ø·ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ (Chunking)\n",
        "        CHUNK_SIZE = 200\n",
        "        OVERLAP = 30\n",
        "\n",
        "        for page in doc_data['pages']:\n",
        "            text = page['clean_text']\n",
        "            words = text.split()\n",
        "            for k in range(0, len(words), CHUNK_SIZE - OVERLAP):\n",
        "                chunk = \" \".join(words[k : k + CHUNK_SIZE])\n",
        "                if len(chunk) > 100:\n",
        "                    dataset_entries.append({\n",
        "                        \"instruction\": f\"Explain the technical concepts in: {doc_data['filename']}\",\n",
        "                        \"input\": \"\",\n",
        "                        \"output\": chunk\n",
        "                    })\n",
        "\n",
        "        # Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙÙŠ Drive\n",
        "        with open(DATASET_FILE, 'w', encoding='utf-8') as f:\n",
        "            for entry in dataset_entries:\n",
        "                json.dump(entry, f, ensure_ascii=False)\n",
        "                f.write('\\n')\n",
        "\n",
        "        print(f\"   âœ… Dataset Saved to Drive: {DATASET_FILE}\")\n",
        "        print(f\"   ğŸ“Š Total High-Quality Samples: {len(dataset_entries)}\")\n",
        "        print(\"\\nğŸ‰ MISSION COMPLETE. Your data is safe in Drive and ready for training.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "id": "cOJpOB9Aft9v",
        "outputId": "c57f9725-c1c3-462f-e2e1-15b1880c03b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ STARTING: Secure Production Pipeline (Drive-Backed)...\n",
            "\n",
            "ğŸ“¤ STEP 1: Upload Document...\n",
            "   ğŸ‘‰ Please upload your Engineering Book (PDF) now:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a5b2838-f7b2-49d0-8176-530ea6606917\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a5b2838-f7b2-49d0-8176-530ea6606917\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Automotive-Engineering-Powertrain-Chassis-System-and-Vehicle-Body-1.pdf to Automotive-Engineering-Powertrain-Chassis-System-and-Vehicle-Body-1.pdf\n",
            "   âœ… Saved to Drive: /content/drive/MyDrive/sovereign-engine-v7.2/data/raw/Automotive-Engineering-Powertrain-Chassis-System-and-Vehicle-Body-1.pdf\n",
            "\n",
            "ğŸ”§ STEP 2: Initializing Engines...\n",
            "\n",
            "âš™ï¸ STEP 3: Processing Automotive-Engineering-Powertrain-Chassis-System-and-Vehicle-Body-1.pdf...\n"
          ]
        }
      ]
    }
  ]
}